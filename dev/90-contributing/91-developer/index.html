<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Developer Documentation · TulipaEnergyModel.jl</title><meta name="title" content="Developer Documentation · TulipaEnergyModel.jl"/><meta property="og:title" content="Developer Documentation · TulipaEnergyModel.jl"/><meta property="twitter:title" content="Developer Documentation · TulipaEnergyModel.jl"/><meta name="description" content="Documentation for TulipaEnergyModel.jl."/><meta property="og:description" content="Documentation for TulipaEnergyModel.jl."/><meta property="twitter:description" content="Documentation for TulipaEnergyModel.jl."/><meta property="og:url" content="https://TulipaEnergy.github.io/TulipaEnergyModel.jl/90-contributing/91-developer/"/><meta property="twitter:url" content="https://TulipaEnergy.github.io/TulipaEnergyModel.jl/90-contributing/91-developer/"/><link rel="canonical" href="https://TulipaEnergy.github.io/TulipaEnergyModel.jl/90-contributing/91-developer/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="TulipaEnergyModel.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TulipaEnergyModel.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../10-tutorials/11-setting-up/">Tutorial Setup</a></li><li><a class="tocitem" href="../../10-tutorials/12-basics/">The Basics</a></li><li><a class="tocitem" href="../../10-tutorials/13-assets/">Gereralized Assets and Flows</a></li><li><a class="tocitem" href="../../10-tutorials/14-fully-flexible-time/">Fully-Flexible Time Resolution</a></li><li><a class="tocitem" href="../../10-tutorials/15-clustering-rep-periods/">Blended Representative Periods with Tulipa Clustering</a></li><li><a class="tocitem" href="../../10-tutorials/16-storage/">Seasonal and Non-seasonal Storage</a></li><li><a class="tocitem" href="../../10-tutorials/17-multi-year-investments/">Multi-Year Investment Pathways</a></li><li><a class="tocitem" href="../../10-tutorials/30-workflow/">Data-Handling Workflow (Example)</a></li><li><a class="tocitem" href="../../10-tutorials/31-rolling-horizon/">Rolling Horizon</a></li><li><a class="tocitem" href="../../10-tutorials/40-bids-workaround/">Bids using Consumer Unit Commitment (Workaround)</a></li><li><a class="tocitem" href="../../10-tutorials/99-manual-steps/">Advanced Control</a></li></ul></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../../20-user-guide/00-getting-started/">Getting Started</a></li><li><a class="tocitem" href="../../20-user-guide/20-how-to-use/">How to Use</a></li><li><a class="tocitem" href="../../20-user-guide/50-workflow/">Analysis Workflow</a></li><li><a class="tocitem" href="../../20-user-guide/54-input-table-schemas/">Input Table Schemas</a></li><li><a class="tocitem" href="../../20-user-guide/55-outputs/">Output Variables</a></li><li><a class="tocitem" href="../../20-user-guide/60-structures/">Internal Model Structures</a></li><li><a class="tocitem" href="../../20-user-guide/65-FFTR-table/">FFTR Constraints Table</a></li></ul></li><li><a class="tocitem" href="../../30-concepts/">Concepts</a></li><li><span class="tocitem">Scientific Foundation</span><ul><li><a class="tocitem" href="../../40-scientific-foundation/40-formulation/">Mathematical Formulation</a></li><li><a class="tocitem" href="../../40-scientific-foundation/45-scientific-references/">Scientific References</a></li></ul></li><li><a class="tocitem" href="../../70-reference/">API Reference</a></li><li><a class="tocitem" href="../../80-ecosystem/">Tulipa Ecosystem</a></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../90-contributing/">Contributing Guidelines</a></li><li class="is-active"><a class="tocitem" href>Developer Documentation</a><ul class="internal"><li><a class="tocitem" href="#First-Time-Setup"><span>First-Time Setup</span></a></li><li><a class="tocitem" href="#Code-format-and-guidelines"><span>Code format and guidelines</span></a></li><li><a class="tocitem" href="#Contributing-Workflow"><span>Contributing Workflow</span></a></li><li><a class="tocitem" href="#Documentation-updates"><span>Documentation updates</span></a></li><li><a class="tocitem" href="#Performance-Considerations"><span>Performance Considerations</span></a></li><li><a class="tocitem" href="#Testing-the-generated-MPS-files"><span>Testing the generated MPS files</span></a></li><li><a class="tocitem" href="#Releasing-a-New-Version"><span>Releasing a New Version</span></a></li><li><a class="tocitem" href="#Adding-a-Package-to-the-TulipaEnergy-Organisation"><span>Adding a Package to the TulipaEnergy Organisation</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Contributing</a></li><li class="is-active"><a href>Developer Documentation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Developer Documentation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/TulipaEnergy/TulipaEnergyModel.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/TulipaEnergy/TulipaEnergyModel.jl/blob/main/docs/src/90-contributing/91-developer.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="developer"><a class="docs-heading-anchor" href="#developer">Developer Documentation</a><a id="developer-1"></a><a class="docs-heading-anchor-permalink" href="#developer" title="Permalink"></a></h1><p>Welcome to TulipaEnergyModel.jl developer documentation. Here is how you can contribute to our Julia-based toolkit for modeling and optimization of electric energy systems.</p><p>This may seem like a lot, but that&#39;s because we walk you through step-by-step. If you follow these steps, you&#39;ll contributing in no time!</p><ul><li><a href="#First-Time-Setup">First-Time Setup</a></li><li class="no-marker"><ul><li><a href="#Installing-Software">Installing Software</a></li><li><a href="#Forking-the-Repository">Forking the Repository</a></li><li><a href="#Configuring-Git">Configuring Git</a></li><li><a href="#Activating-and-Testing-the-Package">Activating and Testing the Package</a></li><li><a href="#Configuring-Linting-and-Formatting">Configuring Linting and Formatting</a></li></ul></li><li><a href="#Code-format-and-guidelines">Code format and guidelines</a></li><li class="no-marker"><ul><li><a href="#Markdown-Table-Formatting-(MD060)">Markdown Table Formatting (MD060)</a></li></ul></li><li><a href="#Contributing-Workflow">Contributing Workflow</a></li><li class="no-marker"><ul><li><a href="#1.-Make-Sure-That-Your-Fork-Is-Up-to-Date">1. Make Sure That Your Fork Is Up to Date</a></li><li><a href="#2.-Create-a-New-Branch">2. Create a New Branch</a></li><li><a href="#3.-Implement-the-Changes">3. Implement the Changes</a></li><li><a href="#4.-Run-the-Tests">4. Run the Tests</a></li><li><a href="#5.-Run-the-Linter">5. Run the Linter</a></li><li><a href="#6.-Commit-the-Changes">6. Commit the Changes</a></li><li><a href="#7.-Make-Sure-That-Your-Fork-Is-Still-Up-to-Date">7. Make Sure That Your Fork Is Still Up to Date</a></li><li><a href="#8.-Create-a-Pull-Request">8. Create a Pull Request</a></li></ul></li><li><a href="#Documentation-updates">Documentation updates</a></li><li class="no-marker"><ul><li><a href="#Building-the-Documentation-Locally">Building the Documentation Locally</a></li></ul></li><li><a href="#Performance-Considerations">Performance Considerations</a></li><li class="no-marker"><ul><li><a href="#Manually-running-the-benchmark-across-versions">Manually running the benchmark across versions</a></li><li><a href="#Investigating-performance-issues">Investigating performance issues</a></li><li><a href="#flame-graph-tips">Flame graph tips</a></li><li><a href="#Testing-scalability">Testing scalability</a></li><li><a href="#Type-instability-investigation">Type instability investigation</a></li><li><a href="#Memory-profiling">Memory profiling</a></li></ul></li><li><a href="#Testing-the-generated-MPS-files">Testing the generated MPS files</a></li><li class="no-marker"><ul><li><a href="#Updating-the-MPS-files">Updating the MPS files</a></li><li><a href="#Comparison-of-MPS-files-via-script">Comparison of MPS files via script</a></li><li><a href="#GitHub-Workflow">GitHub Workflow</a></li></ul></li><li><a href="#Releasing-a-New-Version">Releasing a New Version</a></li><li><a href="#Adding-a-Package-to-the-TulipaEnergy-Organisation">Adding a Package to the TulipaEnergy Organisation</a></li></ul><h2 id="First-Time-Setup"><a class="docs-heading-anchor" href="#First-Time-Setup">First-Time Setup</a><a id="First-Time-Setup-1"></a><a class="docs-heading-anchor-permalink" href="#First-Time-Setup" title="Permalink"></a></h2><p>Before you can start contributing, please read our <a href="../90-contributing/#contributing">Contributing Guidelines</a>.</p><p>Also make sure that you have installed the required software, and that it is properly configured. You only need to do this once.</p><h3 id="Installing-Software"><a class="docs-heading-anchor" href="#Installing-Software">Installing Software</a><a id="Installing-Software-1"></a><a class="docs-heading-anchor-permalink" href="#Installing-Software" title="Permalink"></a></h3><p>To contribute to TulipaEnergyModel.jl, you need the following:</p><ol><li><p><a href="https://julialang.org">Julia</a> programming language.</p></li><li><p><a href="https://git-scm.com">Git</a> for version control.</p></li><li><p><a href="https://code.visualstudio.com">VSCode</a> or any other editor. For VSCode, we recommend to install a few extensions. You can do it by pressing <code>Ctrl + Shift + X</code> (or <code>⇧ + ⌘ + X</code> on MacOS) and searching by the extension name:</p><ul><li><a href="https://www.julia-vscode.org">Julia for Visual Studio Code</a></li><li><a href="https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph">Git Graph</a></li></ul></li><li><p><a href="https://editorconfig.org">EditorConfig</a> for consistent code formatting. In VSCode, it is available as <a href="https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig">an extension</a>.</p></li><li><p><a href="https://pre-commit.com">pre-commit</a> to run the linters and formatters.</p><ul><li>To install pre-commit, you will first need <a href="https://www.python.org/">Python</a> with pip (included by default in recent Python versions).</li></ul><p>You can install <code>pre-commit</code> globally using</p><pre><code class="language-bash hljs">pip install pre-commit</code></pre><p>If you prefer to create a local environment with it, do the following:</p><pre><code class="language-bash hljs">python -m venv env

# Windows
source env/Scripts/activate # in bash
env/Scripts/Activate.ps1 # in powershell

# Linux or macOS
. env/bin/activate

pip install --upgrade pip setuptools pre-commit</code></pre></li><li><p><a href="https://github.com/domluna/JuliaFormatter.jl">JuliaFormatter.jl</a> for code formatting.</p><p>To install it, open a Julia REPL, for example, by typing in the command line:</p><pre><code class="language-bash hljs">julia</code></pre><blockquote><p><strong>Note:</strong> <code>julia</code> must be part of your environment variables to call it from the command line.</p></blockquote><p>Then press <code>]</code> to enter package mode and enter the following:</p><pre><code class="language-julia hljs">pkg&gt; activate
pkg&gt; add JuliaFormatter@v1 # NOTE: We currently have to manually force v1 (see issue #1233)</code></pre><p>In VSCode, you can activate &quot;Format on Save&quot; for <code>JuliaFormatter</code>:</p><ul><li>Open VSCode Settings (<code>Ctrl + ,</code>)</li><li>In &quot;Search Settings&quot;, type &quot;Format on Save&quot; and tick the first result:</li></ul><p><img src="../../images/FormatOnSave.png" alt="Screenshot of Format on Save option"/></p><blockquote><p><strong>Warning</strong>: Do not use the JuliaFormatter VSCode extension because it may conflict with the formatting rules currently used in this codebase.</p></blockquote></li><li><p><a href="https://github.com/JuliaCI/LocalCoverage.jl">LocalCoverage</a> for coverage testing. You can install it the same way you installed <code>JuliaFormatter</code>, that is, by opening Julia REPL in the package mode and typing:</p><pre><code class="language-julia hljs">pkg&gt; activate
pkg&gt; add LocalCoverage</code></pre></li></ol><h3 id="Forking-the-Repository"><a class="docs-heading-anchor" href="#Forking-the-Repository">Forking the Repository</a><a id="Forking-the-Repository-1"></a><a class="docs-heading-anchor-permalink" href="#Forking-the-Repository" title="Permalink"></a></h3><p>Any changes should be done in a <a href="https://docs.github.com/en/get-started/quickstart/fork-a-repo">fork</a>. You can fork this repository directly on GitHub:</p><p><img src="../../images/Fork.png" alt="Screenshot of Fork button on GitHub"/></p><p>After that, clone your fork. The fork should ideally be cloned to a folder that is not affected by any cloud, to prevent LiveServer accessing and overwriting your folder. Finally, add this repository as upstream:</p><pre><code class="language-bash hljs">cd path/to/folder #optionally, cd to a local folder
git clone https://github.com/your-name/TulipaEnergyModel.jl                   # use the fork URL
git remote add upstream https://github.com/TulipaEnergy/TulipaEnergyModel.jl  # use the original repository URL</code></pre><p>Check that your origin and upstream are correct:</p><pre><code class="language-bash hljs">git remote -v</code></pre><p>You should see something similar to: <img src="../../images/Remotes.png" alt="Screenshot of remote names, showing origin and upstream"/></p><p>If your names are wrong, use this command (with the relevant names) to correct it:</p><pre><code class="language-bash hljs">git remote set-url [name] [url]</code></pre><h3 id="Configuring-Git"><a class="docs-heading-anchor" href="#Configuring-Git">Configuring Git</a><a id="Configuring-Git-1"></a><a class="docs-heading-anchor-permalink" href="#Configuring-Git" title="Permalink"></a></h3><p>Because operating systems use different line endings for text files, you need to configure Git to ensure code consistency across different platforms. You can do this with the following commands:</p><pre><code class="language-bash hljs">cd /path/to/TulipaEnergyModel.jl
git config --unset core.autocrlf         # disable autocrlf in the EnergyModel repo
git config --global core.autocrlf false  # explicitly disable autocrlf globally
git config --global --unset core.eol     # disable explicit file-ending globally
git config core.eol lf                   # set Linux style file-endings in EnergyModel</code></pre><h3 id="Activating-and-Testing-the-Package"><a class="docs-heading-anchor" href="#Activating-and-Testing-the-Package">Activating and Testing the Package</a><a id="Activating-and-Testing-the-Package-1"></a><a class="docs-heading-anchor-permalink" href="#Activating-and-Testing-the-Package" title="Permalink"></a></h3><p>Start Julia REPL either via the command line or in the editor.</p><p>In the terminal, do:</p><pre><code class="language-bash hljs">cd /path/to/TulipaEnergyModel.jl  # change the working directory to the repo directory if needed
julia                             # start Julia REPL</code></pre><p>In VSCode, first open your cloned fork as a new project. Then open the command palette with <code>Ctrl + Shift + P</code> (or <code>⇧ + ⌘ + P</code> on MacOS) and use the command called <code>Julia: Start REPL</code>.</p><p>In a Julia REPL, enter the package mode by pressing <code>]</code>.</p><p>In the package mode, first activate and instantiate the project, then run the tests to ensure that everything is working as expected:</p><pre><code class="language-bash hljs">pkg&gt; activate .   # activate the project
(TulipaEnergyModel) pkg&gt; instantiate  # instantiate to install the required packages
(TulipaEnergyModel) pkg&gt; test         # run the tests (uses TestItemRunner.jl)</code></pre><blockquote><p><strong>Note:</strong> The test suite uses TestItemRunner.jl for better test organization and granular execution. You can also run specific test categories using tags or individual test files as described in the testing section below.</p></blockquote><h3 id="Configuring-Linting-and-Formatting"><a class="docs-heading-anchor" href="#Configuring-Linting-and-Formatting">Configuring Linting and Formatting</a><a id="Configuring-Linting-and-Formatting-1"></a><a class="docs-heading-anchor-permalink" href="#Configuring-Linting-and-Formatting" title="Permalink"></a></h3><p>With <code>pre-commit</code> installed, activate it as a pre-commit hook:</p><pre><code class="language-bash hljs">pre-commit install</code></pre><p>To run the linting and formatting manually, enter the command below:</p><pre><code class="language-bash hljs">pre-commit run -a</code></pre><p>Do it once now to make sure that everything works as expected.</p><p>Now, you can only commit if all the pre-commit tests pass.</p><blockquote><p><strong>Note:</strong> On subsequent occasions when you need to run pre-commit in a new shell, you will need to activate the Python virtual environment. If so, do the following:</p><pre><code class="language-bash hljs">. env/bin/activate  # for Windows the command is: . env/Scripts/activate
pre-commit run -a</code></pre></blockquote><h2 id="Code-format-and-guidelines"><a class="docs-heading-anchor" href="#Code-format-and-guidelines">Code format and guidelines</a><a id="Code-format-and-guidelines-1"></a><a class="docs-heading-anchor-permalink" href="#Code-format-and-guidelines" title="Permalink"></a></h2><p>This section will list the guidelines for code formatting <strong>not enforced</strong> by JuliaFormatter. We will try to follow these during development and reviews.</p><ul><li>Naming<ul><li><code>CamelCase</code> for classes and modules</li><li><code>snake_case</code> for functions and variables</li><li><code>kebab-case</code> for file names and documentation reference tags</li></ul></li><li>Use <code>using</code> instead of <code>import</code>, in the following way:<ul><li>Don&#39;t use pure <code>using Package</code>, always list all necessary objects with <code>using Package: A, B, C</code>.</li><li>List obvious objects, e.g., <code>using JuMP: @variable</code>, since <code>@variable</code> is obviously from JuMP in this context.</li><li>For other objects inside <code>Package</code>, use <code>using Package: Package</code> and explicitly call <code>Package.A</code> to use it, e.g., <code>JuMP.direct_model</code>.</li><li>List all <code>using</code> in &lt;src/TulipaEnergyModel.jl&gt;.</li></ul></li><li>Explicitly state what a function will <code>return</code>; if returning nothing, simply use <code>return</code>.</li></ul><h3 id="Markdown-Table-Formatting-(MD060)"><a class="docs-heading-anchor" href="#Markdown-Table-Formatting-(MD060)">Markdown Table Formatting (MD060)</a><a id="Markdown-Table-Formatting-(MD060)-1"></a><a class="docs-heading-anchor-permalink" href="#Markdown-Table-Formatting-(MD060)" title="Permalink"></a></h3><p>Markdown tables in the documentation must follow proper column alignment to pass the MD060 linting rule. This means that all pipes (<code>|</code>) in a table must be vertically aligned, and cells should be properly padded with spaces.</p><p><strong>Example of a properly formatted table:</strong></p><pre><code class="language-markdown hljs">| Column 1 | Column 2 | Column 3 |
| -------- | -------- | -------- |
| Value 1  | Value 2  | Value 3  |
| Data A   | Data B   | Data C   |</code></pre><p><strong>Example of an incorrectly formatted table:</strong></p><pre><code class="language-markdown hljs">| Column 1 | Column 2 | Column 3 |
| --- | --- | --- |
| Value 1 | Value 2 | Value 3 |
| Data A | Data B | Data C |</code></pre><p>If the pre-commit linter fails due to MD060 violations, you have two options:</p><ol><li><p><strong>Manually fix the table</strong>: Adjust the spacing and alignment in your markdown file.</p></li><li><p><strong>Use the automatic formatting script</strong>: Run the Python script located in <code>utils/apply_md_rule.py</code> to automatically format all tables in a markdown file:</p><pre><code class="language-bash hljs">python utils/apply_md_rule.py path/to/your/file.md</code></pre><p>For example, to fix tables in the formulation documentation:</p><pre><code class="language-bash hljs">python utils/apply_md_rule.py docs/src/40-scientific-foundation/40-formulation.md</code></pre><p>The script will automatically detect all tables in the file and apply proper alignment.</p></li></ol><h2 id="Contributing-Workflow"><a class="docs-heading-anchor" href="#Contributing-Workflow">Contributing Workflow</a><a id="Contributing-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#Contributing-Workflow" title="Permalink"></a></h2><p>When the software is installed and configured, and you have forked the TulipaEnergyModel.jl repository, you can start contributing to it.</p><p>We use the following workflow for all contributions:</p><ol><li>Make sure that your fork is up to date</li><li>Create a new branch</li><li>Implement the changes</li><li>Run the tests</li><li>Run the linter</li><li>Commit the changes</li><li>Repeat steps 3-6 until all necessary changes are done</li><li>Make sure that your fork is still up to date</li><li>Create a pull request</li></ol><p>Below you can find detailed instructions for each step.</p><h3 id="1.-Make-Sure-That-Your-Fork-Is-Up-to-Date"><a class="docs-heading-anchor" href="#1.-Make-Sure-That-Your-Fork-Is-Up-to-Date">1. Make Sure That Your Fork Is Up to Date</a><a id="1.-Make-Sure-That-Your-Fork-Is-Up-to-Date-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Make-Sure-That-Your-Fork-Is-Up-to-Date" title="Permalink"></a></h3><p>Fetch from org remote, fast-forward your local main:</p><pre><code class="language-bash hljs">git switch main
git fetch --all --prune
git merge --ff-only upstream/main</code></pre><blockquote><p><strong>Warning</strong>: If you have a conflict on your main, it will appear now. You can delete your old <code>main</code> branch using</p><pre><code class="language-bash hljs">git reset --hard upstream/main</code></pre></blockquote><h3 id="2.-Create-a-New-Branch"><a class="docs-heading-anchor" href="#2.-Create-a-New-Branch">2. Create a New Branch</a><a id="2.-Create-a-New-Branch-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Create-a-New-Branch" title="Permalink"></a></h3><p>Create a branch to address the issue:</p><pre><code class="language-bash hljs">git switch -c &lt;branch_name&gt;</code></pre><ul><li>If there is an associated issue, add the issue number to the branch name, for example, <code>123-short-description</code> for issue #123.</li><li>If there is no associated issue <strong>and the changes are small</strong>, add a prefix such as &quot;typo&quot;, &quot;hotfix&quot;, &quot;small-refactor&quot;, according to the type of update.</li><li>If the changes are not small and there is no associated issue, then create the issue first, so we can properly discuss the changes.</li></ul><blockquote><p><strong>Note:</strong> Always branch from <code>main</code>, i.e., the main branch of your own fork.</p></blockquote><h3 id="3.-Implement-the-Changes"><a class="docs-heading-anchor" href="#3.-Implement-the-Changes">3. Implement the Changes</a><a id="3.-Implement-the-Changes-1"></a><a class="docs-heading-anchor-permalink" href="#3.-Implement-the-Changes" title="Permalink"></a></h3><p>Implement your changes to address the issue associated with the branch.</p><h3 id="4.-Run-the-Tests"><a class="docs-heading-anchor" href="#4.-Run-the-Tests">4. Run the Tests</a><a id="4.-Run-the-Tests-1"></a><a class="docs-heading-anchor-permalink" href="#4.-Run-the-Tests" title="Permalink"></a></h3><p>TulipaEnergyModel.jl uses <a href="https://github.com/julia-vscode/TestItemRunner.jl">TestItemRunner.jl</a> for testing, which provides granular test execution and better development experience.</p><h4 id="Running-All-Tests"><a class="docs-heading-anchor" href="#Running-All-Tests">Running All Tests</a><a id="Running-All-Tests-1"></a><a class="docs-heading-anchor-permalink" href="#Running-All-Tests" title="Permalink"></a></h4><p>In Julia:</p><pre><code class="language-bash hljs">TulipaEnergyModel&gt; test</code></pre><p>Or, using the CLI version:</p><pre><code class="language-bash hljs">julia --project=test test/runtests.jl --verbose</code></pre><h4 id="Running-Tests-with-filters"><a class="docs-heading-anchor" href="#Running-Tests-with-filters">Running Tests with filters</a><a id="Running-Tests-with-filters-1"></a><a class="docs-heading-anchor-permalink" href="#Running-Tests-with-filters" title="Permalink"></a></h4><p>The test suite uses tags for efficient test filtering during development. Here are some tags:</p><ul><li><code>:unit</code> - Single component or function tests</li><li><code>:integration</code> - End-to-end tests</li><li><code>:validation</code> - Tests verifying expected values, behavior, or mathematical correctness</li><li><code>:fast</code> - Quick tests suitable for frequent execution</li><li><code>:slow</code> - Resource-intensive tests requiring significant time or memory</li></ul><p>Check <code>test/runtests.jl</code> for a complete list or run with the <code>--list-tags</code> option.</p><p>Examples of running specific test categories:</p><pre><code class="language-bash hljs"># Run only fast unit tests
julia --project=test test/runtests.jl --tags unit,fast

# Run only files that match the name:
julia --project=test test/runtests.jl --file test-model.jl</code></pre><p>And it&#39;s also possible to run a specific <code>@testitem</code>. If you are on VSCode with the Julia extension installed, you will see a &quot;play&quot; button on the left side of the <code>@testitem</code>. Otherwise, if you know (part of) the name of the test, you can run it with:</p><pre><code class="language-bash hljs">julia --project=test test/runtests.jl --name &quot;Some test name&quot;</code></pre><p>Use <code>--help</code> to see the full help.</p><h4 id="Test-Coverage"><a class="docs-heading-anchor" href="#Test-Coverage">Test Coverage</a><a id="Test-Coverage-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Coverage" title="Permalink"></a></h4><p>To run the tests with code coverage, you can use the <code>LocalCoverage</code> package:</p><pre><code class="language-julia hljs">julia&gt; using LocalCoverage
# ]
pkg&gt; activate .
# &lt;backspace&gt;
julia&gt; cov = generate_coverage()</code></pre><p>This will run the tests, track line coverage and print a report table as output. Note that we want to maintain 100% test coverage. If any file does not show 100% coverage, please add tests to cover the missing lines.</p><p>If you are having trouble reaching 100% test coverage, you can set your pull request to &#39;draft&#39; status and ask for help.</p><h4 id="Writing-New-Tests"><a class="docs-heading-anchor" href="#Writing-New-Tests">Writing New Tests</a><a id="Writing-New-Tests-1"></a><a class="docs-heading-anchor-permalink" href="#Writing-New-Tests" title="Permalink"></a></h4><p>When adding new tests, use the <code>@testitem</code> macro with appropriate setup and tags:</p><pre><code class="language-julia hljs">@testitem &quot;Test description&quot; setup = [CommonSetup] tags = [:unit, :fast] begin
    # Test code here
    @test actual_result == expected_result
end</code></pre><p>Choose appropriate tags based on your test characteristics. If you need to add a new tag, check the <code>TAGS_DATA</code> dictionary on the <code>test/runtests.jl</code> file.</p><h3 id="5.-Run-the-Linter"><a class="docs-heading-anchor" href="#5.-Run-the-Linter">5. Run the Linter</a><a id="5.-Run-the-Linter-1"></a><a class="docs-heading-anchor-permalink" href="#5.-Run-the-Linter" title="Permalink"></a></h3><p>In the bash/git bash terminal, run pre-commit:</p><pre><code class="language-bash hljs">. env/bin/activate # if necessary (for Windows the command is: . env/Scripts/activate)
pre-commit run -a</code></pre><p>If any of the checks failed, find in the pre-commit log what the issues are and fix them. Then, add them again (<code>git add</code>), rerun the tests &amp; linter, and commit.</p><h3 id="6.-Commit-the-Changes"><a class="docs-heading-anchor" href="#6.-Commit-the-Changes">6. Commit the Changes</a><a id="6.-Commit-the-Changes-1"></a><a class="docs-heading-anchor-permalink" href="#6.-Commit-the-Changes" title="Permalink"></a></h3><p>When the test are passing, commit the changes and push them to the remote repository. Use:</p><pre><code class="language-bash hljs">git commit -am &quot;A short but descriptive commit message&quot; # Equivalent to: git commit -a -m &quot;commit msg&quot;
git push -u origin &lt;branch_name&gt;</code></pre><p>When writing the commit message:</p><ul><li>use imperative, present tense (Add feature, Fix bug);</li><li>have informative titles;</li><li>if necessary, add a body with details.</li></ul><blockquote><p><strong>Note:</strong> Try to create &quot;atomic git commits&quot;. Read <a href="https://blog.esciencecenter.nl/the-utopic-git-history-d44b81c09593"><em>The Utopic Git History</em></a> to learn more.</p></blockquote><h3 id="7.-Make-Sure-That-Your-Fork-Is-Still-Up-to-Date"><a class="docs-heading-anchor" href="#7.-Make-Sure-That-Your-Fork-Is-Still-Up-to-Date">7. Make Sure That Your Fork Is Still Up to Date</a><a id="7.-Make-Sure-That-Your-Fork-Is-Still-Up-to-Date-1"></a><a class="docs-heading-anchor-permalink" href="#7.-Make-Sure-That-Your-Fork-Is-Still-Up-to-Date" title="Permalink"></a></h3><p>If necessary, fetch any <code>main</code> updates from upstream and rebase your branch into <code>origin/main</code>. For example, do this if it took some time to resolve the issue you have been working on. If you don&#39;t resolve conflicts locally, you will get conflicts in your pull request.</p><p>Do the following steps:</p><pre><code class="language-bash hljs">git switch main                  # switch to the main branch
git fetch --all --prune          # fetch the updates
git merge --ff-only upstream/main  # merge as a fast-forward
git switch &lt;branch_name&gt;         # switch back to the issue branch
git rebase main &lt;branch_name&gt;    # rebase it</code></pre><p>If it says that you have conflicts, resolve them by opening the file(s) and editing them until the code looks correct to you. You can check the changes with:</p><pre><code class="language-bash hljs">git diff             # Check that changes are correct.
git add &lt;file_name&gt;
git diff --staged    # Another way to check changes, i.e., what you will see in the pull request.</code></pre><p>Once the conflicts are resolved, commit and push.</p><pre><code class="language-bash hljs">git status # Another way to show that all conflicts are fixed.
git rebase --continue
git push --force origin &lt;branch_name&gt;</code></pre><h3 id="8.-Create-a-Pull-Request"><a class="docs-heading-anchor" href="#8.-Create-a-Pull-Request">8. Create a Pull Request</a><a id="8.-Create-a-Pull-Request-1"></a><a class="docs-heading-anchor-permalink" href="#8.-Create-a-Pull-Request" title="Permalink"></a></h3><p>When there are no more conflicts and all the test are passing, create a pull request to merge your remote branch into the org main. You can do this on GitHub by opening the branch in your fork and clicking &quot;Compare &amp; pull request&quot;.</p><p><img src="../../images/CompareAndPR.png" alt="Screenshot of Compare &amp; pull request button on GitHub"/></p><p>Fill in the pull request details:</p><ol><li>Describe the changes.</li><li>List the issue(s) that this pull request closes.</li><li>Fill in the collaboration confirmation.</li><li>(Optional) Choose a reviewer.</li><li>When all of the information is filled in, click &quot;Create pull request&quot;.</li></ol><p><img src="../../images/PRInfo.png" alt="Screenshot of the pull request information"/></p><p>You pull request will appear in the list of pull requests in the TulipaEnergyModel.jl repository, where you can track the review process.</p><p>Sometimes reviewers request changes. After pushing any changes, the pull request will be automatically updated. Do not forget to re-request a review.</p><p>Once your reviewer approves the pull request, you need to merge it with the main branch using &quot;Squash and Merge&quot;. You can also delete the branch that originated the pull request by clicking the button that appears after the merge. For branches that were pushed to the main repo, it is recommended that you do so.</p><h4 id="Tests-on-Pull-Requests"><a class="docs-heading-anchor" href="#Tests-on-Pull-Requests">Tests on Pull Requests</a><a id="Tests-on-Pull-Requests-1"></a><a class="docs-heading-anchor-permalink" href="#Tests-on-Pull-Requests" title="Permalink"></a></h4><p>The pull request tests are run by the workflow <code>TestOnPRs</code>, while the tests on <code>main</code> are run by the <code>Test</code> workflow. The <code>Test</code> workflow is much more comprehensive, running the tests on two Julia versions (Latest and Long Term Support) and 3 operating systems (Linux, MacOS, Windows).</p><p>The <code>TestOnPRs</code> workflow, by default, only runs the tests using the latest Julia version and on Linux. By tagging the pull request with the label <code>pr-test-all</code>, the <code>TestOnPRs</code> workflow changes behaviour to run the tests on all 3 OSs mentioned above.</p><h2 id="Documentation-updates"><a class="docs-heading-anchor" href="#Documentation-updates">Documentation updates</a><a id="Documentation-updates-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation-updates" title="Permalink"></a></h2><p>When updating the documentation, pull requests mades from branches within the repo (i.e., not from forks) will have a preview build enabled and a preview link pasted on the comments. The PR preview link will look something like: <code>https://tulipaenergy.github.io/TulipaEnergyModel.jl/previews/PRXXXX</code>.</p><p>Alternatively, you might want to build the documentation locally. See below for more information.</p><h3 id="Building-the-Documentation-Locally"><a class="docs-heading-anchor" href="#Building-the-Documentation-Locally">Building the Documentation Locally</a><a id="Building-the-Documentation-Locally-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-Documentation-Locally" title="Permalink"></a></h3><p>Following the latest suggestions, we recommend using <code>LiveServer</code> to build the documentation.</p><blockquote><p><strong>Note</strong>: Ensure you have the package <code>Revise</code> installed in your global environment before running <code>servedocs</code>.</p></blockquote><p>Here is how you do it:</p><ol><li>Run <code>julia --project=docs</code> in the package root to open Julia in the environment of the docs.</li><li>If this is the first time building the docs<ol><li>Press <code>]</code> to enter <code>pkg</code> mode</li><li>Run <code>pkg&gt; dev .</code> to use the development version of your package</li><li>Press backspace to leave <code>pkg</code> mode</li></ol></li><li>Run <code>julia&gt; using LiveServer</code></li><li>Run <code>julia&gt; servedocs(launch_browser=true)</code></li></ol><h2 id="Performance-Considerations"><a class="docs-heading-anchor" href="#Performance-Considerations">Performance Considerations</a><a id="Performance-Considerations-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Considerations" title="Permalink"></a></h2><p>If you updated something that might impact the performance of the package, we expect that the <code>Benchmark.yml</code> workflow runs for your pull request. To do that, add the tag <code>benchmark</code> in the pull request. This will trigger the workflow and post the results as a comment in you pull request.</p><blockquote><p><strong>Warning</strong>: This requires that your branch was pushed to the main repo. If you have created a pull request from a fork, the Benchmark.yml workflow does not work. Instead, close your pull request, push your branch to the main repo, and open a new pull request.</p></blockquote><p>If you want to manually run the benchmarks, you can do the following:</p><ul><li><p>Run <code>julia --project=benchmark</code> from the root folder</p></li><li><p>Enter <code>pkg</code> mode by pressing <code>]</code> and <code>pkg&gt; instantiate</code>  of <code>pkg&gt; update</code> if necessary</p></li><li><p>Now run</p><pre><code class="language-julia hljs">include(&quot;benchmark/benchmarks.jl&quot;)
tune!(SUITE)
results = run(SUITE, verbose=true)</code></pre></li><li><p>To compare these results to a different run, say <code>results_old</code>, use</p><pre><code class="language-julia hljs">judge(old=results_old, new=results)</code></pre></li></ul><h3 id="Manually-running-the-benchmark-across-versions"><a class="docs-heading-anchor" href="#Manually-running-the-benchmark-across-versions">Manually running the benchmark across versions</a><a id="Manually-running-the-benchmark-across-versions-1"></a><a class="docs-heading-anchor-permalink" href="#Manually-running-the-benchmark-across-versions" title="Permalink"></a></h3><p>We use the package <a href="https://github.com/MilesCranmer/AirspeedVelocity.jl">AirspeedVelocity.jl</a> to run the benchmarks in the CI, but it can also be used to compare explicitly named versions manually.</p><ol><li><p>Run the following to install AirspeedVelocity&#39;s commands to your Julia <code>bin</code> folder (<code>~/.julia/bin</code> on MacOS and Linux). On Windows, if you are using the default Julia installation, search for <code>C:/Users/</code> then the folder of your Windows user and then <code>.julia/bin</code></p><pre><code class="language-bash hljs">julia -e &#39;using Pkg; Pkg.activate(temp=true); Pkg.add(&quot;AirspeedVelocity&quot;)&#39;</code></pre></li><li><p>Check that <code>benchpkg</code> was installed:</p><pre><code class="language-bash hljs">benchpkg --version</code></pre><p>If if can&#39;t be found, then it is possible that your Julia <code>bin</code> folder is not in the <code>PATH</code>. After fixing this, try again.</p></li><li><p>Then, run <code>benchpkg</code> with <code>--rev</code> to list the versions to be tested and <code>--bench-on</code> to indicate with script to use (if necessary). For instance:</p><pre><code class="language-bash hljs">benchpkg TulipaEnergyModel --rev=v0.12.0,main --bench-on=main</code></pre><p>After all logging, the output should look like</p><pre><code class="language-plaintext hljs">|                                      | v0.12.0       | main            | v0.12.0/main |
|:-------------------------------------|:-------------:|:---------------:|:------------:|
| energy_problem/create_model          | 25.1 ± 1.2 s  | 19.7 ± 1.1 s    | 1.27         |
| energy_problem/input_and_constructor | 11.2 ± 0.15 s | 8.57 ± 0.064 s  | 1.3          |
| time_to_load                         | 1.7 ± 0.022 s | 1.73 ± 0.0055 s | 0.979        |</code></pre><p>Be aware that the versions passed in <code>rev</code> must be compatible to the benchmark defined at <code>bench-on</code>. So, for instance, testing <code>v0.10.4</code> above would fail, before the versions are too different.</p><p>If you are working on a local version of <code>TulipaEnergyModel</code>, it is possible to test the local modifications. First, make sure that you are at the root of the <code>TulipaEnergyModel</code> repo, and then issue</p><pre><code class="language-bash hljs">benchpkg --rev=&lt;other&gt;,dirty</code></pre><p>The <code>dirty</code> value refers to the current local modifications. The <code>&lt;other&gt;</code> values can be tags or branches to compare.</p></li><li><p>When this is done, you can print just the table afterwards using <code>benchmarktable</code>:</p><pre><code class="language-bash hljs">benchpkgtable TulipaEnergyModel --rev=v0.12.0,main
...
|                                      | v0.12.0       | main            | v0.12.0/main |
|:-------------------------------------|:-------------:|:---------------:|:------------:|
| energy_problem/create_model          | 25.1 ± 1.2 s  | 19.7 ± 1.1 s    | 1.27         |
| energy_problem/input_and_constructor | 11.2 ± 0.15 s | 8.57 ± 0.064 s  | 1.3          |
| time_to_load                         | 1.7 ± 0.022 s | 1.73 ± 0.0055 s | 0.979        |</code></pre></li><li><p>It is also possible to generate a plot, using <code>benchpkgplot</code>:</p><pre><code class="language-bash hljs">benchpkgplot TulipaEnergyModel --rev=v0.12.0,main --format=jpeg</code></pre><p>Different formats can be used. Here is the output:</p><p><img src="../../images/plot_TulipaEnergyModel.jpeg" alt="Plot of benchmark made with benchpkgplot"/></p></li></ol><h3 id="Investigating-performance-issues"><a class="docs-heading-anchor" href="#Investigating-performance-issues">Investigating performance issues</a><a id="Investigating-performance-issues-1"></a><a class="docs-heading-anchor-permalink" href="#Investigating-performance-issues" title="Permalink"></a></h3><p>Make sure to check <a href="https://modernjuliaworkflows.org/optimizing/">Modern Julia Workflows</a> at least until measurements.</p><p>When investigating performance issues, we use three main ways to check out (speed) performance of functions:</p><ul><li>Run the pipeline until the relevant part and check the <code>TimerOutput</code> log.<ul><li>TulipaEnergyModel holds a global <code>TimerOutput</code>. This strategy is to simply run the relevant parts of the pipeline and <code>show(TEM.to)</code> to see the results.</li><li>You can also use <code>TimerOutputs.reset_timer!</code> to reset the timer manually, which can be useful to limit the log.</li><li>Check <code>benchmark/profiling/timer-output.jl</code>.</li></ul></li><li>Benchmark the relevant part, preparing a setup function.<ul><li>Create a <code>setup</code> function that generates everything necessary for the function you&#39;re benchmarking</li><li>If you&#39;re benchmarking more than one function, wrap them in a function</li><li>Call <code>@benchmark</code> with a <code>setup</code> argument</li><li>Check <code>benchmark/profiling/benchmarktools.jl</code></li></ul></li><li>Use <code>@profview</code> for flame graph profiling.<ul><li>Reuse the code from above.</li><li>Call the setup function and then call <code>@profview</code> on the function that you&#39;re investigating.</li><li>This needs to be done in VSCode, or using the ProfileView package.</li><li>This will create a flame graph, where each function call is a block. The size of the block is proportional to the aggregate time it takes to run. The blocks below a block are functions called inside the function above.</li><li>Check <code>benchmark/profiling/profview.jl</code>.</li><li>See the <a href="#flame-graph-tips">flame graph tips</a> below for more details.</li></ul></li></ul><p>In all cases, you can run the relevant function (after inspecting it) in the <code>benchmark</code> folder environment:</p><pre><code class="language-bash hljs">julia --project=&quot;benchmark/profiling&quot;
# press ]
pkg&gt; instantiate
# press backspace
julia&gt; include(&quot;benchmark/profiling/relevant-file.jl&quot;)</code></pre><h3 id="flame-graph-tips"><a class="docs-heading-anchor" href="#flame-graph-tips">Flame graph tips</a><a id="flame-graph-tips-1"></a><a class="docs-heading-anchor-permalink" href="#flame-graph-tips" title="Permalink"></a></h3><p>When running <code>@profview</code> in VSCode, a tab will appear with the flame graph. If running outside VSCode, you need the <a href="https://github.com/timholy/ProfileView.jl">ProfileView</a> package.</p><p>The flame graph will normally have many functions (~15) from the Julia side <em>before</em> the actual code. This is normally identified by an <code>eval</code> block. Click on a block to zoom into that region, and focus on the TEM code.</p><h4 id="Too-fast"><a class="docs-heading-anchor" href="#Too-fast">Too fast</a><a id="Too-fast-1"></a><a class="docs-heading-anchor-permalink" href="#Too-fast" title="Permalink"></a></h4><p>Make sure that your <code>@profview</code> call is not too fast. You want to have your code run for enough time that the sampler will capture relevant information. If you have a larger data input, that is better. Otherwise, run the relevant code inside a loop.</p><h4 id="Focus"><a class="docs-heading-anchor" href="#Focus">Focus</a><a id="Focus-1"></a><a class="docs-heading-anchor-permalink" href="#Focus" title="Permalink"></a></h4><p>If the part of the code that you want to profile does not appear in the flame graph and you are already running in a loop, then you need to change your benchmark to run something more focused.</p><p>Look into the tests, maybe there is already something that can be reused.</p><h4 id="Large-blocks"><a class="docs-heading-anchor" href="#Large-blocks">Large blocks</a><a id="Large-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Large-blocks" title="Permalink"></a></h4><p>The size of the blocks is proportional to how much the code is taking to execute (according to the sampler). If a block is large, it is relevant, but that doesn&#39;t mean that it&#39;s wrong. In a large scale problem, some things will be slow.</p><h4 id="Color-codes"><a class="docs-heading-anchor" href="#Color-codes">Color codes</a><a id="Color-codes-1"></a><a class="docs-heading-anchor-permalink" href="#Color-codes" title="Permalink"></a></h4><p>The color of the blocks are an indication of other problems in the code.</p><p>There are various tonalities of blue, which is normal.</p><p>The red blocks are bad. It means, in essence, that there are issues related to type. We should try to avoid these as much as possible, although sometimes it will happen in other packages (e.g. DataFrames).</p><p>The yellow blocks are also bad. It means that the garbage collector was called, which means that some memory stopped being used. E.g., creating a temporary array.</p><p>See the <a href="https://github.com/timholy/ProfileView.jl?tab=readme-ov-file#color-coding">ProfileView color coding documentation</a> for more information.</p><h4 id="Repeated-blocks"><a class="docs-heading-anchor" href="#Repeated-blocks">Repeated blocks</a><a id="Repeated-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Repeated-blocks" title="Permalink"></a></h4><p>Sometimes a block will be small but will appear in many places. Every time that the block appears, it&#39;s a separate execution. This means that the actual time of the code is the aggregate of all blocks. These are great candidates for improvement if they add up to become a large block. However, as with the large blocks, being slow does not mean that it&#39;s wrong.</p><h3 id="Testing-scalability"><a class="docs-heading-anchor" href="#Testing-scalability">Testing scalability</a><a id="Testing-scalability-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-scalability" title="Permalink"></a></h3><p>This is still a new topic for us, so material is scarce. At the moment, check <code>benchmark/profiling/scalability.jl</code> for an example of running a benchmark on many artificial Tulipa problems, with varying sizes, saving the results, and creating a plot out of it. You can see an example of the expected output in the file <code>results.csv</code> and the plot <code>results.png</code> in the same folder.</p><h3 id="Type-instability-investigation"><a class="docs-heading-anchor" href="#Type-instability-investigation">Type instability investigation</a><a id="Type-instability-investigation-1"></a><a class="docs-heading-anchor-permalink" href="#Type-instability-investigation" title="Permalink"></a></h3><p>To investigate type instability issues in the code, we can use <code>@code_warntype</code>, <a href="https://github.com/aviatesk/JET.jl">JET.jl</a>, and <a href="https://github.com/JuliaDebug/Cthulhu.jl">Cthulhu.jl</a>, in increasing complexity order.</p><p>If you have a single function that you can directly call, <code>@code_warntype</code> might be enough to investigate possible type instability issues. Most times, though, the function will be deeply nested, so using JET or Cthulhu will be necessary to actually see what is going on. See the <code>benchmark/profiling/type-stability.jl</code> script for an example of setting up the lower or higher level API and calling some of these functions.</p><p>Check <a href="https://modernjuliaworkflows.org/optimizing/#type_stability">Modern Julia Workflow&#39;s type stability section</a> for more details.</p><h3 id="Memory-profiling"><a class="docs-heading-anchor" href="#Memory-profiling">Memory profiling</a><a id="Memory-profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-profiling" title="Permalink"></a></h3><p>To investigate memory usage of the code, we use the <a href="https://docs.julialang.org/en/v1/manual/profile/#allocation-profiler">Allocation Profiler</a>. The script <code>benchmark/profiling/memory-profile.jl</code> has an example using PProf. It should be possible to use VSCode&#39;s <a href="https://www.julia-vscode.org/docs/stable/userguide/profiler/"><code>@profview_allocs</code></a> as well.</p><p>Notice that the <code>sample_rate</code> value might be relevant in this investigation, though at the moment we don&#39;t have a recommendation on how to find the best value.</p><p>Profiling memory use is useful to figure out <em>where</em> large amounts of memory are being allocated, but not exactly <em>why</em>. Type stability is one of the main issues. Check <a href="https://modernjuliaworkflows.org/optimizing/#type_stability">Modern Julia Workflows&#39; type stability section</a> and memory management section for more information.</p><h2 id="Testing-the-generated-MPS-files"><a class="docs-heading-anchor" href="#Testing-the-generated-MPS-files">Testing the generated MPS files</a><a id="Testing-the-generated-MPS-files-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-the-generated-MPS-files" title="Permalink"></a></h2><p>To make sure that unintended changes don&#39;t change the model, we have a workflow that automatically compares the generated MPS files. Here is an explanation of how it works, and how to run the same comparison locally.</p><p>Before we start, notice that there are files in <code>benchmark/model-mps-folder</code> with the <code>.mps</code> files for each of the test inputs. There are the <em>existing</em> MPS files.</p><h3 id="Updating-the-MPS-files"><a class="docs-heading-anchor" href="#Updating-the-MPS-files">Updating the MPS files</a><a id="Updating-the-MPS-files-1"></a><a class="docs-heading-anchor-permalink" href="#Updating-the-MPS-files" title="Permalink"></a></h3><p>To update the MPS files, you can simple run a script from the root of <code>TulipaEnergyModel.jl</code>:</p><pre><code class="language-bash hljs">julia --project=. utils/scripts/model-mps-update.jl</code></pre><p>If you <strong>know</strong> that your changes will modify the model, then you need to update the MPS files as just showed.</p><h3 id="Comparison-of-MPS-files-via-script"><a class="docs-heading-anchor" href="#Comparison-of-MPS-files-via-script">Comparison of MPS files via script</a><a id="Comparison-of-MPS-files-via-script-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-of-MPS-files-via-script" title="Permalink"></a></h3><p>One quick way to check the difference between the existing MPS files and the new ones is just to run the update, and check the <code>git diff</code>. However, if you don&#39;t want to update, or just want a summary of the changes, you can run the script:</p><pre><code class="language-bash hljs">julia --project=. utils/scripts/model-mps-compare.jl</code></pre><div class="admonition is-warning" id="Warning-7e3f4e0aba700056"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-7e3f4e0aba700056" title="Permalink"></a></header><div class="admonition-body"><p>This comparison uses the <em>local version of <code>benchmark/model-mps-folder</code></em>. So, if you run the update script, there will be no changes to be shown.</p></div></div><p>The generated log will look something liek this:</p><pre><code class="language-plaintext hljs">┌ Info: New comparison
│ Comparing files
│ - &lt;path&gt;/&lt;file&gt;.mps
└ - &lt;temp-path&gt;/&lt;file&gt;.mps
[ Info: Create mps for &lt;path&gt; in &lt;temp-path&gt;
[ Info: No difference found
┌ Info: New comparison
│ Comparing files
│ - &lt;path&gt;/&lt;file&gt;.mps
└ - &lt;temp-path&gt;/&lt;file&gt;.mps
[ Info: Create mps for &lt;path&gt; in &lt;temp-path&gt;
┌ Error: Line 1272&quot;
│ ..Existing: &#39;    assets_investment[2030,ocgt] max_output_flows_limit[ocgt,2030,1,18:18] -100&#39;
│ .......New: &#39;    assets_investment[2030,ocgt] max_output_flows_limit[ocgt,2030,1,18:18] -200&#39;
└ @ Main &lt;path&gt;/utils/scripts/model-mps-compare.jl:75</code></pre><p>There are 2 cases:</p><ol><li>The first case starts at the beginning of the log and ends in &quot;No difference found&quot;. There was nothing to show for that file.</li><li>The second case has &quot;errors&quot;, i.e., differences between the existing and new MPS files. Here is what to expect from the error lines:<ul><li><code>Error: Line ####</code>: The line number of the MPS file (which you can manually inspect).</li><li><code>..Existing</code>: Shows the existing line.</li><li><code>.......New</code>: Shows the new line.</li></ul></li></ol><p>If the environment variable <code>TULIPA_COMPARE_MPS_LOGFILE</code> is defined and is a path to a file, then the log will be written to a file instead of printed. This is mostly relevant for the GitHub workflow.</p><h3 id="GitHub-Workflow"><a class="docs-heading-anchor" href="#GitHub-Workflow">GitHub Workflow</a><a id="GitHub-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#GitHub-Workflow" title="Permalink"></a></h3><p>When creating a pull request, the workflow <code>CompareMPS.yml</code> will run the comparison above and write a PR comment to indicate whether the files are the same or not. If the files are not the same, then the workflow fails, and there are two ways in which the workflow can fail:</p><ol><li><strong>Expected failure</strong>: If you are making a change to the model, then the MPS file will be different. Then you should 1.1. Verify that the changes are <strong>only</strong> what you expected to see (i.e., use the MPS difference to debug possible issues). 1.2. Run the update script listed above to fix the comparison (i.e., the new MPS becomes the existing MPS). 1.3. Commit and push your modifications and wait for the comparison to run again.</li><li><strong>Unexpected failure</strong>: If you made modifications that were not supposed to change the model, then you need to investigate what happened. Use the MPS difference to debug what you have done. There is no easy fix for this. If you think there are bugs in the comparison script, discuss with your PR reviewer and open an issue if necessary.</li></ol><div class="admonition is-warning" id="Warning-87a8c1586b27d0b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-87a8c1586b27d0b8" title="Permalink"></a></header><div class="admonition-body"><p>The comparison workflow only writes PR comments if the branch is made from within <code>TulipaEnergyModel</code> (i.e., not from forks). To see the log online in that case, you have to open the GitHub action log, or run the comparison locally, as explained in the previous section.</p></div></div><h2 id="Releasing-a-New-Version"><a class="docs-heading-anchor" href="#Releasing-a-New-Version">Releasing a New Version</a><a id="Releasing-a-New-Version-1"></a><a class="docs-heading-anchor-permalink" href="#Releasing-a-New-Version" title="Permalink"></a></h2><p>When publishing a new version of the model to the Julia Registry, follow this procedure:</p><div class="admonition is-info" id="Note-d0d3deac05ab798a"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d0d3deac05ab798a" title="Permalink"></a></header><div class="admonition-body"><p>To be able to register, you need to be a member of the organisation TulipaEnergy and set your visibility to public: <img src="../../images/PublicMember.png" alt="Screenshot of public members of TulipaEnergy on GitHub"/></p></div></div><ol><li><p>Click on the <code>Project.toml</code> file on GitHub.</p></li><li><p>Edit the file and change the version number according to <a href="https://semver.org/">semantic versioning</a>: Major.Minor.Patch</p><p><img src="../../images/UpdateVersion.png" alt="Screenshot of editing Project.toml on GitHub"/></p></li><li><p>Commit the changes in a new branch and open a pull request. Change the commit message according to the version number.</p><p><img src="../../images/CommitMessage.png" alt="Screenshot of PR with commit message &quot;Release 0.6.1&quot;"/></p></li><li><p>Create the pull request and squash &amp; merge it after the review and testing process. Delete the branch after the squash and merge.</p><p><img src="../../images/PullRequest.png" alt="Screenshot of full PR template on GitHub"/></p></li><li><p>Go to the main page of repo and click in the commit. <img src="../../images/AccessCommit.png" alt="Screenshot of how to access commit on GitHub"/></p></li><li><p>Add the following comment to the commit: <code>@JuliaRegistrator register</code></p><p><img src="../../images/JuliaRegistrator.png" alt="Screenshot of calling JuliaRegistrator in commit comments"/></p></li><li><p>The bot should start the registration process.</p><p><img src="../../images/BotProcess.png" alt="Screenshot of JuliaRegistrator bot message"/></p></li><li><p>After approval, the bot will take care of the PR at the Julia Registry and automatically create the release for the new version.</p><p><img src="../../images/NewRelease.png" alt="Screenshot of new version on registry"/></p><p>Thank you for helping make frequent releases!</p></li></ol><h2 id="Adding-a-Package-to-the-TulipaEnergy-Organisation"><a class="docs-heading-anchor" href="#Adding-a-Package-to-the-TulipaEnergy-Organisation">Adding a Package to the TulipaEnergy Organisation</a><a id="Adding-a-Package-to-the-TulipaEnergy-Organisation-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-a-Package-to-the-TulipaEnergy-Organisation" title="Permalink"></a></h2><p>To get started creating a new (Julia) package that will live in the TulipaEnergy organisation and interact with TulipaEnergyModel, please start by using <a href="https://github.com/JuliaBesties/BestieTemplate.jl">BestieTemplate.jl</a>, and follow the steps in their <a href="https://juliabesties.github.io/BestieTemplate.jl/stable/10-guides/10-full-guide">Full guide</a> for a new package.</p><p>This will set up the majority of automation and workflows we use and make your repo consistent with the others!</p><blockquote><p><strong>Note:</strong> TulipaEnergyModel.jl is the core repo of the organisation. The Discussions are focused there and in some cases the documentation of other packages should forward to the TulipaEnergyModel docs to avoid duplicate or scattered information.</p></blockquote></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../90-contributing/">« Contributing Guidelines</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 11 February 2026 17:05">Wednesday 11 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
